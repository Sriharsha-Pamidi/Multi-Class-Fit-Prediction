{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6905bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from collections import defaultdict\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5668a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562d0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "dataset = pd.read_json('cse-258-project/renttherunway_final_data.json.gz', lines=True)\n",
    "dataset = dataset.dropna()\n",
    "dataset = dataset.drop(dataset[dataset['rented for']== \"party: cocktail\"].index)\n",
    "\n",
    "# 0 = Small, 1 = Fit, 2 = Large\n",
    "dataset.loc[dataset[\"fit\"] == \"small\", \"fit\"] = 0\n",
    "\n",
    "dataset.loc[dataset[\"fit\"] == \"fit\", \"fit\"] = 1\n",
    "\n",
    "dataset.loc[dataset[\"fit\"] == \"large\", \"fit\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f3ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/rv0p_vz52hzcccft68rtk3840000gn/T/ipykernel_9089/1164200574.py:2: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  data = pd.DataFrame(dataset).to_dict('record')\n"
     ]
    }
   ],
   "source": [
    "# Converting the data into list of dictionaries\n",
    "data = pd.DataFrame(dataset).to_dict('record')\n",
    "\n",
    "for d in data:\n",
    "    d['weight'] = int(d['weight'].split('lbs')[0])\n",
    "    d['height'] = int(d['height'].split(' ')[0][:-1])*12 + int(d['height'].split(' ')[1][:-1])\n",
    "#     if(int(d['height'].split(' ')[1].split('\"')[0])<10):\n",
    "#         height2 = int(d['height'].split(' ')[1].split('\"')[0])*10\n",
    "#     else:\n",
    "#         height2 = int(d['height'].split(' ')[1].split('\"')[0])\n",
    "#     d['height'] = int(d['height'][0][0])*100+height2\n",
    "    \n",
    "####converting categroies type to one hot\n",
    "catogeries = ['rented for','body type']\n",
    "for cat in catogeries:\n",
    "    categories_list = defaultdict(int)\n",
    "    for d in data:\n",
    "        categories_list[d[cat]] += 1\n",
    "        \n",
    "    categories_id = defaultdict(int)\n",
    "\n",
    "    i = 0\n",
    "    for cID in  categories_list:\n",
    "        categories_id[cID] = i\n",
    "        i+=1\n",
    "    for d in data:\n",
    "        f = [0]*len(categories_list)\n",
    "        f[categories_id[d[cat]]] = 1\n",
    "        d[cat] = f[:len(categories_list)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825435a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76727f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb933aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3b151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.50d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43357fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "\n",
    "# def load_vectors(fname):\n",
    "#     fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, fin.readline().split())\n",
    "#     data = {}\n",
    "#     for line in fin:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         data[tokens[0]] = map(float, tokens[1:])\n",
    "#     return data\n",
    "\n",
    "# embeddings_dict = load_vectors(\"wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "604a951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(word):\n",
    "    try:\n",
    "        return embeddings_dict[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca295e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings_dict['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c850956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features\n",
    "df = []\n",
    "for d in data:\n",
    "    arr = []\n",
    "#     arr.append(d['weight'])\n",
    "    arr.append(d['rating'])\n",
    "    arr += d['rented for']\n",
    "    arr += list(sum([get_word_embeddings(word) for word in d['review_text'].split()])/len([get_word_embeddings(word) for word in d['review_text'].split()]))\n",
    "    try:\n",
    "        arr += list(sum([get_word_embeddings(word) for word in d['review_summary'].split()])/(len([get_word_embeddings(word) for word in d['review_summary'].split()])+1))\n",
    "    except:\n",
    "        arr += [0]*50\n",
    "    \n",
    "#     arr.append(len(d['review_text']))\n",
    "    arr += d['body type']\n",
    "#     arr.append(d['height'])\n",
    "    arr.append(d['size'])\n",
    "#     arr.append(d['age'])\n",
    "    df.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7e33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[\"fit\"]\n",
    "y_cat = to_categorical(y)\n",
    "split_ratio = .85\n",
    "# Split data\n",
    "test_y = y_cat[int(len(y_cat) * split_ratio):]\n",
    "train_y = y_cat[:int(len(y_cat) * split_ratio)]\n",
    "test_f = df[int(len(y_cat) * split_ratio):]\n",
    "train_f = df[:int(len(y_cat) * split_ratio)]\n",
    "\n",
    "# Convert to numpy array.\n",
    "test_f = np.array(test_f)\n",
    "train_f = np.array(train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "160c8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2e61e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 60)                6960      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                2440      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 123       \n",
      "=================================================================\n",
      "Total params: 9,523\n",
      "Trainable params: 9,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 08:01:01.510906: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(60, input_shape = (len(train_f[0]), ), activation = \"relu\"))\n",
    "model.add(Dense(40, activation = \"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(3, activation = \"softmax\"))\n",
    "model.compile(Adam(lr = 0.001), \"categorical_crossentropy\", metrics = ['acc',f1_m,precision_m, recall_m])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a3a76bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 08:01:01.857587: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3889/3889 [==============================] - 5s 1ms/step - loss: 0.7031 - acc: 0.7392 - f1_m: 0.7332 - precision_m: 0.7610 - recall_m: 0.7087\n",
      "Epoch 2/10\n",
      "3889/3889 [==============================] - 3s 814us/step - loss: 0.6839 - acc: 0.7431 - f1_m: 0.7364 - precision_m: 0.7673 - recall_m: 0.7090\n",
      "Epoch 3/10\n",
      "3889/3889 [==============================] - 3s 748us/step - loss: 0.6777 - acc: 0.7445 - f1_m: 0.7380 - precision_m: 0.7689 - recall_m: 0.7106\n",
      "Epoch 4/10\n",
      "3889/3889 [==============================] - 3s 710us/step - loss: 0.6734 - acc: 0.7459 - f1_m: 0.7400 - precision_m: 0.7692 - recall_m: 0.7140\n",
      "Epoch 5/10\n",
      "3889/3889 [==============================] - 3s 709us/step - loss: 0.6700 - acc: 0.7467 - f1_m: 0.7413 - precision_m: 0.7691 - recall_m: 0.7164\n",
      "Epoch 6/10\n",
      "3889/3889 [==============================] - 3s 784us/step - loss: 0.6671 - acc: 0.7475 - f1_m: 0.7427 - precision_m: 0.7703 - recall_m: 0.7181\n",
      "Epoch 7/10\n",
      "3889/3889 [==============================] - 3s 759us/step - loss: 0.6650 - acc: 0.7485 - f1_m: 0.7431 - precision_m: 0.7704 - recall_m: 0.7186\n",
      "Epoch 8/10\n",
      "3889/3889 [==============================] - 3s 769us/step - loss: 0.6626 - acc: 0.7496 - f1_m: 0.7445 - precision_m: 0.7724 - recall_m: 0.7194\n",
      "Epoch 9/10\n",
      "3889/3889 [==============================] - 3s 728us/step - loss: 0.6605 - acc: 0.7498 - f1_m: 0.7447 - precision_m: 0.7725 - recall_m: 0.7198\n",
      "Epoch 10/10\n",
      "3889/3889 [==============================] - 3s 801us/step - loss: 0.6582 - acc: 0.7511 - f1_m: 0.7460 - precision_m: 0.7735 - recall_m: 0.7213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19ee913a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_f, train_y, verbose=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c768f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "# y_pred_class = model.predict_classes(test_f)\n",
    "predict_x=model.predict(test_f) \n",
    "y_pred_class=np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3f4b89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  367,  2517,    90],\n",
       "       [  204, 15829,   128],\n",
       "       [  142,  2413,   267]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(test_f)\n",
    "y_test_class = np.argmax(test_y, axis=1)\n",
    "confusion_matrix(y_test_class, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24d97ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7498"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_class, y_pred_class).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00f470fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6788"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test_class, y_pred_class, average='weighted').round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8cf161f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7498"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_test_class, y_pred_class, average='weighted').round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfc80881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7017"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test_class, y_pred_class, average='weighted').round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f6f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35a573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
